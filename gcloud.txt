//To copy files over from cloud bucket
#################################################

gsutil cp gs://my-bucket/*.txt
gsutil cp *.txt gs://my-bucket





















































































######################################################################################

Restore my bashrc

cp /etc/skel/.bashrc ~/


Creating a Bucket

gsutil mb gs://<BUCKET_NAME>


######################################################################################


To set my region instead of google asking me everytime

gcloud compute regions list

INFRACLASS_REGION=[YOUR_REGION]$HOME

echo $INFRACLASS_REGION


######################################################################################


Resetting Cloud Shell to default state

ls -a $HOME
sudo rm -rf $HOME

Then Click restart cloud shell after clicking the three dot icon

####
ls -a 
sudo rm -rf $HOME

Then Click restart cloud shell after clicking the three dot icon


######################################################################################


Create a persistent state in Cloud Shell

mkdir infraclas


touch infraclass/config

echo INFRACLASS_REGION=$INFRACLASS_REGION >> ~/infraclass/config

INFRACLASS_PROJECT_ID=[YOUR_PROJECT_ID]

echo INFRACLASS_PROJECT_ID=$INFRACLASS_PROJECT_ID >> ~/infraclass/config

source infraclass/config
echo $INFRACLASS_PROJECT_ID

nano .profile

source infraclass/config

echo $INFRACLASS_PROJECT_ID



#######################################################################################3
Jenkins bitman deploy manager

user
uyhWJBdFfTQ8 
http://34.83.1.234/jenkins/


After ssh into the machine

sudo /opt/bitnami/ctlscript.sh stop   //shut down all running services
sudo /opt/bitnami/ctlscript.sh restart //Restart the services

#######################################################################################3

Virtual Cloud Networks 


ping learn-1  // learn-1 is a vm-instance and while in the ssh of that vm, The items can only be pinged with the vm-name if it is in the same vm

ping learn-2  // This won't work because learn-2 is another vm and you are trying to ping an internal ip.
            //The VM learn-2 is not in the default network where learn-1 is located. So the symbolic name can't be translated.

            
ping <learn-2's internal ip> This will work in some cases because different vm's could have the same ip address. As a result you are actually pinging your own local VM's interface

ping <learn-2's external IP>  //This works

sudo apt-get install traceroute  

sudo traceroute google.com -I   //To check if google.com is reachable. -I is ICMP Echo for probes

sudo traceroute <learn-2's external IP> -I  //To find path to learn-2's external IP

    //Recall that learn-2 is in an auto-mode network, so firewall rules were automatically created that enabled ingress traffic to reach its external IP. However, learn-3 is in a custom-mode network, and no firewall rules were established. You created a firewall rule to permit access.
    
gcloud compute networks subnets expand-ip-range new-uswest  --prefix-length 23  --region us-west1 //To ioncrease the address range, run the following command


sudo apt-get update //update the package index

sudo apt-get install apache2 -y  //install a simple web application on your instance to represent an internal application

echo '<!doctype html><html><body><h1>Hello World!</h1></body></html>' 
| sudo tee /var/www/html/index.html     //To create a new default web page by overwriting the default, run the following  


curl webserver     //To verify that home page on webserver is reachable from bastion

ssh -a webserver  //To connect to webserver. In this case webserver was a VM

/*When instances do not have external IP addresses, they can only be reached by other instances on the network or via a managed VPN gateway.
In this case, the bastion VM serves as a management and maintenance interface to the webserver VM.*/


#######################################################################################
Creating Virtual Machines

Create severak standard VMs
Create advanced VMS

free    //To see information about unsued and used memory and swap space on your custom VM, run the following command

sudo dmidecode -t 17  //To see details about the RAM installed on your VM, run the following command

nproc           //To verify the number of processors, run the following


lscpu           //To see details about the CPUs installed on your VM

#######################################################################################

Prepare data disk

To create a directory and format and mount the disk

sudo mkdir -p /home/minecraft         //To create a directory that serves as the mount point for the data disk, run the following command

sudo mkfs.ext4 -F -E lazy_itable_init=0,\       //To format the disk, run the following
lazy_journal_init=0,discard \
/dev/disk/by-id/google-minecraft-disk


sudo mount -o discard,defaults /dev/disk/by-id/google-minecraft-disk /home/minecraft //To mount the disk


Install the java RunTime Environment and the minecraft server


sudo apt-get update
sudo apt-get install -y default-jre-headless  //To install headless JRE
sudo wget https://s3.amazonaws.com/Minecraft.Download/versions/1.11.2/minecraft_server.1.11.2.jar   //To download the current Minecraft server JAR file(1.11.2 JAR) 

Initialize the Minecraft server


sudo java -Xms1G -Xmx7G -d64 -jar minecraft_server.1.11.2.jar nogui //To initalize the Minecraft server, run the following command

sudo vi eula.txt  /To edit the Eula to true

Create a virtual terminal screen to start the Minecraft server

sudo apt-get install -y screen   //to install screen

sudo screen -S mcs java -Xms1G -Xmx7G -d64 -jar /home/minecraft/minecraft_server.1.11.2.jar nogui //To start your Minecraft server in a screen virtual terminal

CTRl+A,D   //To detach the screen terminal

sudo screen -r mcs    //To reattach the terminal


To Schedule regular backups


export YOUR_BUCKET_NAME=<Enter your bucket name here> //To create a globally unique bucket name and store it in the environment variable

gsutil mb gs://$YOUR_BUCKET_NAME-minecraft-backup      //To create the bucket using the gsutil tool


Create a backup script

sudo vim /home/minecraft/backup.sh

Shell Script

#!/bin/bash
screen -r mcs -X stuff '/save-all\n/save-off\n'
/usr/bin/gsutil cp -R ${BASH_SOURCE%/*}/world gs://${YOUR_BUCKET_NAME}-minecraft-backup/$(date "+%Y%m%d-%H%M%S")-world
screen
 -r mcs -X stuff '/save-on\n'

/* The script saves the current state of the server's world data and pauses the server's auto-save functionality. Next, it backs up the server's world data directory (world), placing its contents in a timestamped directory (<timestamp>-world) in the Cloud Storage bucket. After the script finishes backing up the data, it resumes auto-saving on the Minecraft server
*/


sudo chmod 755 /home/minecraft/backup.sh   //To make the script executable
. /home/minecraft/backup.sh                //To run the backup script


Schedule a cron job to automate the task

sudo crontab -e        //To open cron table for editing


0 */4 * * * /home/minecraft/backup.sh      //At the bottom of the cron table paste.
//This line instracts cron to run backups every 4 hours

sudo 
 -r -X stuff '/stop\n'     //To stop the server

/*
Instead of going through the manual process to mount the persistent disk and launch the server application in a screen, you can use metadata scripts to create a startup script and a shutdown script to do this for you. */

In vm-instance custom metadata 

For key, type startup-scriptnjghkk

For Value 
		
#!/bin/bash
mount /dev/disk/by-id/google-minecraft-disk /home/minecraft
(crontab -l ; echo "0 */4 * * * /home/minecraft/backup.sh")| crontab -
cd /home/minecraft
screen -d -m -S mcs java -Xms1G -Xmx7G -d64 -jar minecraft_server.1.11.2.jar nogui

For key, type shutdown-script
For Value

#!/bin/bash
sudo screen -r -X stuff '/stop\n'



chmod 400 ~/.ssh/[KEY_FILENAME]


//Setting an ssh from terminal
https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys

gcloud compute --project "aiunlv-218808" ssh --zone "us-west2-a" "instance-1" //Loggin in to a vm



#################################################################################################
Cloud Storage Bucket















#################################################################################################
Link to tensor flow pricing calculator


https://cloud.google.com/products/calculator/ 

https://arxiv.org/pdf/1606.07792.pdf     

gcloud AI modules    //lists your models



#################################################################################################

Cloud Sql


After setting up the Cloud sql

go to vm-instance

sudo apt-get update

sudo apt-get install mysql-client

To setup database access

export IP_ADDRESS_SQL=<Enter IP address of CloudSQL instance here>  //Get the public ip address from the console 
mysql --host=$IP_ADDRESS_SQL -u root -p  //To connect to mysql I created
	show databases;          //Listing all databases
	exit;                    //Exit the database server to return to vm-instance
curl -O https://storage.googleapis.com/cloud-training/archinfra/Example-billing-export.csv //To download a sample csv file

mysql --host=$IP_ADDRESS_SQL -u root -p
	create databse resource;  //To create a databse resource
	use resources;            //Use resources

Create a table billing in the resource database

	CREATE TABLE billing
	(
	Account_ID varchar(255),
	Line_Item varchar(255),
	Start_Time varchar(255),
	End_Time varchar(255),
	Project varchar(255),
	Measurement1 varchar(255),
	Measurement1_Total_Consumption bigint(255),
	Measurement1_Units varchar(255),
	Cost varchar(255),
	Currency int,
	Project_Number varchar(255),
	Project_ID varchar(255),
	Project_Name varchar(255),
	Project_Labels varchar(255),
	Description varchar(255)
	);
	
	show tables;
	describe billing;
	exit;

My sql requires that the csv file has the same name as the table. 

mv Example-billing-export.csv billing.csv

mysqlimport --host=$IP_ADDRESS_SQL --ignore-lines=1 --fields-terminated-by=, resources --verbose --local -u root -p  billing.csv  //to Load data into the table


mysql --host=$IP_ADDRESS_SQL -u root -p    //To return to the mysql client

	use resources;
	describe billing;
	SELECT Measurement1, Cost from billing;  //Verify the data is now in the database
	exit;

Setup ssl encryption

mysql -uroot -p -h $IP_ADDRESS_SQL \
    --ssl-ca=server-ca.pem \
    --ssl-cert=client-cert.pem \
    --ssl-key=client-key.pem

show databases;      //Verify the connection by listing all databases on the Cloud SQL instance

#################################################################################################
Big Query Commands for search filtering


SELECT * from Task   //To filter using Query by GQL
SELECT * from Task  //To filter using Query by GQL

//Display rows with more than 0 costs

SELECT * FROM `imported_billing_data.sampleinfotable`
WHERE Cost > 0



//For New Query, paste the following in Query Editor

SELECT
  product,
  resource_type,
  start_time,
  end_time,
  cost,
  project_id,
  project_name,
  project_labels_key,
  currency,
  currency_conversion_rate,
  usage_amount,
  usage_unit
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`

//To find the latest 100 records where there were charges(cost > 0), for New Query, paste the following in Query Editor

SELECT
  product,
  resource_type,
  start_time,
  end_time,
  cost,
  project_id,
  project_name,
  project_labels_key,
  currency,
  currency_conversion_rate,
  usage_amount,
  usage_unit
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
WHERE
  Cost > 0
ORDER BY end_time DESC
LIMIT
  100

//To find all charges that were more than 3 dollars, for Compose New Query, paste the following in Query Editor:

SELECT
  product,
  resource_type,
  start_time,
  end_time,
  cost,
  project_id,
  project_name,
  project_labels_key,
  currency,
  currency_conversion_rate,
  usage_amount,
  usage_unit
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
WHERE
  cost > 3

// To find the product with the most records in the billing data for New Query.

SELECT
  product,
  resource_type,
  start_time,
  end_time,
  cost,
  project_id,
  project_name,
  project_labels_key,
  currency,
  currency_conversion_rate,
  usage_amount,
  usage_unit
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
WHERE
  cost > 3

//To find the most frequently used product costing more than 1 dollar, for New Query, paste the following in Query Editor

SELECT
  product,
  COUNT(*) AS billing_records
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
WHERE
  cost > 1
GROUP BY
  product
ORDER BY
  billing_records DESC

//To find the most commonly charged unit of measure, for Compose New Query, paste the following in Query Editor

SELECT
  usage_unit,
  COUNT(*) AS billing_records
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
WHERE cost > 0
GROUP BY
  usage_unit
ORDER BY
  billing_records DESC

//To find the product with the highest aggregate cost, for New Query

SELECT
  product,
  ROUND(SUM(cost),2) AS total_cost
FROM
  `cloud-training-prod-bucket.arch_infra.billing_data`
GROUP BY
  product
ORDER BY
  total_cost DESC


#################################################################################################

//To create 3 nginx instances using a Bitnami Nginx Stack image, which includes a complete PHP, MySql, and Ngnix development environment

for i in {1..3}; \
do \
  gcloud compute instances create "nginxstack-$i" \
  --machine-type "f1-micro" \
  --tags nginxstack-tcp-443,nginxstack-tcp-80 \
  --zone us-central1-f \
  --image   "https://www.googleapis.com/compute/v1/projects/bitnami-launchpad/global/images/bitnami-nginx-1-14-0-4-linux-debian-9-x86-64" \
  --boot-disk-size "200" --boot-disk-type "pd-standard" \
  --boot-disk-device-name "nginxstack-$i"; \
done//

//To create a firewall rule to allow external traffic to the instances

gcloud compute firewall-rules create nginx-firewall \
 --allow tcp:80,tcp:443 \
 --target-tags nginxstack-tcp-80,nginxstack-tcp-443

#################################################################################################
Deploying an application //Used for Error Reporting


//To create a local folder and get the App Engine Hello World application
mkdir appengine-hello
cd appengine-hello
gsutil cp gs://cloud-training/archinfra/gae-hello/*


dev_appserver.py $(pwd)    //To run the application using the local development server in Cloud Shell

gcloud app deploy app.yaml  //To deploy the application to App Engine

gcloud app browse //To get a link to the application


sed -i -e 's/webapp2/webapp22/' main.py  //To switch the import library to the nonexistant webapp22 

cat main.py           //To verify the change you made in the main.py

gcloud app deploy app.yaml --quiet   //To re-deploy the application to App engine. --quiet it avoides the need for you to type y when prompted to continue deployment


#################################################################################################
Virtual private Cloud

ping -c 3 <Enter server-1's external IP here>   //To checck if you can reach your other vm. it will say 0% packet loss

gcloud config list project     //To check if gcloud is working.


gcloud compute target-vpn-gateways \
create vpn-1 \
--network vpn-network-1  \
--region us-central1               //To create a vpn gateway

gcloud compute addresses create --region us-central1 vpn-1-static-ip  //To reserve a static ip for the vpn

gcloud compute addresses list   //To view the Static IP for the vpn-1 gateway

export STATIC_IP_VPN_1=<Enter IP address for vpn-1 here>  //To store the static Ip for the vpn-1 gatewy, in an environment variable and replace the ip address with the address from the output of the last command

gcloud compute addresses create --region europe-west1 vpn-2-static-ip  //To reserve a Static Ip for the vpn-2 gateway

gcloud compute addresses list //To view the Static Ip for the vpn-2 gateway

The forwarding rules forward traffic arriving on the external IP to the VPN gateway,It connects them together. Create three forwarding rules for the protocols necessary.

gcloud compute \
forwarding-rules create vpn-1-esp \
--region us-central1  \
--ip-protocol ESP  \
--address $STATIC_IP_VPN_1 \
--target-vpn-gateway vpn-1     //To create the ESP forwarding for vpn-1


gcloud compute \
forwarding-rules create vpn-2-esp \
--region europe-west1  \
--ip-protocol ESP  \
--address $STATIC_IP_VPN_2 \
--target-vpn-gateway vpn-2   //To create the vpn-2 


gcloud compute \
forwarding-rules create vpn-1-udp500  \
--region us-central1 \
--ip-protocol UDP \
--ports 500 \
--address $STATIC_IP_VPN_1 \
--target-vpn-gateway vpn-1 //To create UDP500 forwarding for vpn-1



gcloud compute \
forwarding-rules create vpn-2-udp500  \
--region europe-west1 \
--ip-protocol UDP \
--ports 500 \
--address $STATIC_IP_VPN_2 \
--target-vpn-gateway vpn-2    //To create UDP500 forwarding for vpn-2


gcloud compute \
forwarding-rules create vpn-1-udp4500  \
--region us-central1 \
--ip-protocol UDP --ports 4500 \
--address $STATIC_IP_VPN_1 \
--target-vpn-gateway vpn-1   //To create UDP4500 forwarding


gcloud compute \
forwarding-rules create vpn-2-udp4500  \
--region europe-west1 \
--ip-protocol UDP --ports 4500 \
--address $STATIC_IP_VPN_2 \
--target-vpn-gateway vpn-2    //To create UDP4500 forwarding

gcloud compute target-vpn-gateways list   //In the cloud shell to verify VPN gateways


gcloud compute \
vpn-tunnels create tunnel1to2  \
--peer-address $STATIC_IP_VPN_2 \
--region us-central1 \
--ike-version 2 \
--shared-secret gcprocks \
--target-vpn-gateway vpn-1 \
--local-traffic-selector 0.0.0.0/0 \
--remote-traffic-selector 0.0.0.0/0   //To create the tunnel for traffic from the Network-1 to Network-2 


gcloud compute \
vpn-tunnels create tunnel2to1 \
--peer-address $STATIC_IP_VPN_1 \
--region europe-west1 \
--ike-version 2 \
--shared-secret gcprocks \
--target-vpn-gateway vpn-2 \
--local-traffic-selector 0.0.0.0/0 \
--remote-traffic-selector 0.0.0.0/0   //To create the tunnel for traffic from Network-2 to Network-1 


gcloud compute vpn-tunnels list  //To verify that the tunnels are created


gcloud compute  \
routes create route1to2  \
--network vpn-network-1 \
--next-hop-vpn-tunnel tunnel1to2 \
--next-hop-vpn-tunnel-region us-central1 \
--destination-range 10.1.3.0/24   //To create the static routes from Network-1 to Network-2

gcloud compute  \
routes create route2to1  \
--network vpn-network-2 \
--next-hop-vpn-tunnel tunnel2to1 \
--next-hop-vpn-tunnel-region europe-west1 \
--destination-range 10.5.4.0/24    //To create a static route from Network-2 to Network-1


ping -c 3 <internal ip address> //if 0% packet loss then it was able to connect to server-1's internal ip




#################################################################################################

Configure the external load balancer

gcloud compute regions list  //Lists the available regions
gcloud compute zones list   //list the available zone

export MY_REGION=us-central1
export MY_ZONE1=us-central1-a
export MY_ZONE2=us-central1-f


gcloud compute target-pools create extloadbalancer \
    --region $MY_REGION --http-health-check webserver-health  //to create an external load balancer

gcloud compute target-pools add-instances extloadbalancer \
    --instances webserver1,webserver2,webserver3 \
     --instances-zone=$MY_ZONE1                        //Add the three VMs into the target pool exloadbalancer



gcloud compute addresses list      //lists the address of your Virtual private cloud. To get the reserved static IP address


export STATIC_EXTERNAL_IP=[YOUR_STATIC_IP]     //To create an environment variable for the static external IP listed run the following


gcloud compute forwarding-rules create webserver-rule \
    --region $MY_REGION --ports 80 \
    --address $STATIC_EXTERNAL_IP --target-pool extloadbalancer    //To complete the configuration of the forwarding rule


while true; do curl -m1 $STATIC_EXTERNAL_IP; done    //To send repeated requests to the external load balancer


//To launch new webserver4 in the second zone

gcloud compute instances create webserver4 \
    --image-family debian-9 \
    --image-project debian-cloud \
    --tags int-lb \
    --zone $MY_ZONE2 \
    --subnet default \
    --metadata startup-script-url="gs://cloud-training/archinfra/mystartupscript",my-server-id="WebServer-4"

//To launch webserver5 in the second zone

gcloud compute instances create webserver5 \
    --image-family debian-9 \
    --image-project debian-cloud \
    --tags int-lb \
    --zone $MY_ZONE2 \
    --subnet default \
    --metadata startup-script-url="gs://cloud-training/archinfra/mystartupscript",my-server-id="WebServer-5"



gcloud compute instance-groups unmanaged create ig1 \          
    --zone $MY_ZONE1                                   //To create instance group for each zone


gcloud compute instance-groups unmanaged add-instances ig1 \
    --instances=webserver2,webserver3 --zone $MY_ZONE1           //To add webserver2 and webserver3 to ig1

***
Internal load balancing spreads incoming connections based on the session affinity setting. If session affinity is not set, the load balancer spreads all connections across all available instances as evenly as possible, regardless of current load.***


gcloud compute health-checks create tcp my-tcp-health-check \
    --port 80                                                  //To create a health check  //Configure the load balancer *internal load balancer


gcloud compute backend-services create my-int-lb \
    --load-balancing-scheme internal \
    --region $MY_REGION \
    --health-checks my-tcp-health-check \
    --protocol tcp                                     //To create a backend service


gcloud compute backend-services add-backend my-int-lb \
    --instance-group ig1 \
    --instance-group-zone $MY_ZONE1 \
    --region $MY_REGION                                //To add ig1 to your backend service

gcloud compute forwarding-rules create my-int-lb-forwarding-rule \
    --load-balancing-scheme internal \
    --ports 80 \
    --network default \
    --subnet default \
    --region $MY_REGION \
    --backend-service my-int-lb   ..To create a forwarding rule


gcloud compute firewall-rules create allow-internal-lb \
    --network default \
    --source-ranges 0.0.0.0/0 \
    --target-tags int-lb \
    --allow tcp:80,tcp:443                 //To configure a firewall rule to allow traffic to load balancer and from the load balancer to the instance


         
gcloud compute firewall-rules create allow-health-check \
    --network default \
    --source-ranges 130.211.0.0/22,35.191.0.0/16 \
    --target-tags int-lb \
    --allow tcp                                  //To configure another firewall rule to allow health check probesfrom the health checker


gcloud compute instances create standalone-instance-1 \
    --image-family debian-9 \
    --image-project debian-cloud \
    --zone $MY_ZONE1 \
    --tags standalone \
    --subnet default              //For testing purposes,create a standalone client instance in the same region as the load balancer



gcloud compute firewall-rules create allow-ssh-to-standalone \
    --network default \
    --target-tags standalone \
    --allow tcp:22                 //To create a firewall rule to allow SSH connectinos to the standalone client instance


default (10.128.0.0/20)	10.128.0.7:80	


export ILB_IP=<Enter the IP address of the internal load balancer>   //Store the IP address of the internal load balancer

for ((i=1;i<=10;i++)); do curl $ILB_IP; done       //Curl the IP address of the load balancer several times

#################################################################################################

AutoScaling

sudo apt-get update
sudo apt-get install -y apache2    //To install apache2
sudo service apache2 start         //To start the apache server

sudo a2ensite default-ssl
sudo a2enmod ssl
sudo service apache2 restart      //To enable SSL and restart the apache server

sudo update-rc.d apache2 enable  //To set the service to start on boot

sudo service apache2 status      //You can check the server by connecting via SSH to the VM

ab -n 50000 -c 1000 http://$LB_IP/  //To place a load on the load balancer













